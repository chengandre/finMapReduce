_type: prompt
input_variables: [context]
template: |
  <template>
    {{
      "evaluation_results": [
        {{
          "evaluation_number": 1,
          "reasoning": "string with reasoning for the answer",
          "judgement": "string with the answer"
        }},
        {{
          "evaluation_number": 2,
          "reasoning": "...",
          "judgement": "..."
        }},
        {{
          "evaluation_number": 3,
          "reasoning": "...",
          "judgement": "..."
        }},
        {{
          "evaluation_number": 4,
          "reasoning": "...",
          "judgement": "..."
        }},
        {{
          "evaluation_number": 5,
          "reasoning": "...",
          "judgement": "..."
        }}
      ]
    }}
  </template>

  <examples>
  Example Correct:
  {{
    "evaluation_number": 1,
    "reasoning": "The LLM answer matched the golden answer's value of $125M, included all key assumptions, and only had a minor rounding diff ($125.1M).",
    "judgement": "Correct"
  }}

  Example Incorrect:
  {{
    "evaluation_number": 1,
    "reasoning": "Golden answer is $3.5M but LLM gave $2.2M. This is not a rounding issue and the underlying reasoning is based on different assumptions.",
    "judgement": "Incorrect"
  }}

  Example No answer:
  {{
    "evaluation_number": 1,
    "reasoning": "The LLM explicitly said 'Not enough information to calculate,' so no attempt at the answer was made.",
    "judgement": "No answer"
  }}
  </examples>

  <role>
  Act as a methodical financial analyst, supporting every conclusion with step-by-step reasoning and verifiable evidence.
  </role>

  <task>
  Your task is to judge if the LLM answer matches the given golden answer given the financial question.

  You are given 5 triples of (LLM answer, golden answer, query) in <context>. Based solely on the provided context, you must evaluate each of the 5 triples.

  Evaluation Criteria:
  For each triple, your judgement must be one of the following three strings:
    - "Correct": If the LLM answer matches the golden answer or is partially correct. Small rounding errors are allowed. If the LLM answer is a subset of the golden answer, it is still considered correct.
    - "No answer": If the LLM refused to answer the query or stated there was not enough information.
    - "Incorrect": If the LLM answer attempts to answer the query but does not match the golden answer.

  Output Format:
  You MUST provide your response as a single, valid JSON object. Do not write any text or explanations outside of the JSON object itself.

  The JSON object must contain one key, "evaluation_results", which holds an array of 5 objects. Each object in the array represents the evaluation for one of the triples and must contain exactly these three keys:
    - evaluation_number: The integer number of the triple being evaluated (from 1 to 5).
    - reasoning: A string explaining your reasoning for the judgement.
    - judgement: Your evaluation, which must be one of the three strings defined in the criteria above.

  Example in <template> tag.
  </task>

  <context>
  {context}
  </context>