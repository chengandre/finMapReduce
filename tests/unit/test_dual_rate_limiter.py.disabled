import time
import threading
import pytest
from unittest.mock import patch
import sys
import os

# Add parent directory to path to import src.utils.document_processing as utils
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.llm.async_llm_client import AsyncDualRateLimiter, TokenEstimator


class TestDualRateLimiter:
    """Test suite for DualRateLimiter class"""

    def test_initialization(self):
        """Test that DualRateLimiter initializes correctly"""
        config = {
            'requests_per_minute': 120,
            'tokens_per_minute': 150000,
            'request_burst_size': 20
        }

        limiter = DualRateLimiter(config)

        assert limiter.requests_per_minute == 120
        assert limiter.tokens_per_minute == 150000
        assert limiter.request_burst_size == 20
        assert limiter.token_burst_size == int(150000 / 60 * 2)  # 2 seconds of tokens
        assert limiter.request_bucket == 20.0
        assert limiter.token_bucket == float(limiter.token_burst_size)

    def test_default_token_burst_size(self):
        """Test that token_burst_size is auto-calculated when not provided"""
        config = {
            'requests_per_minute': 60,
            'tokens_per_minute': 90000
        }

        limiter = DualRateLimiter(config)

        expected_token_burst = int(90000 / 60 * 2)  # 2 seconds worth of tokens
        assert limiter.token_burst_size == expected_token_burst
        assert limiter.request_burst_size == 10  # default

    def test_custom_token_burst_size(self):
        """Test that custom token_burst_size is respected"""
        config = {
            'requests_per_minute': 60,
            'tokens_per_minute': 90000,
            'token_burst_size': 5000
        }

        limiter = DualRateLimiter(config)

        assert limiter.token_burst_size == 5000

    def test_instant_permission_with_available_resources(self):
        """Test that permission is granted instantly when resources are available"""
        config = {
            'requests_per_minute': 60,
            'tokens_per_minute': 6000,
            'request_burst_size': 5,
            'token_burst_size': 100
        }

        limiter = DualRateLimiter(config)

        start_time = time.time()
        limiter.wait_for_permission(50)
        end_time = time.time()

        # Should be instant (less than 100ms)
        assert (end_time - start_time) < 0.1

        # Check that resources were consumed
        assert limiter.request_bucket == 4.0  # 5 - 1
        assert limiter.token_bucket == 50.0   # 100 - 50

    def test_request_rate_limiting(self):
        """Test that request rate limiting works correctly"""
        config = {
            'requests_per_minute': 60,  # 1 per second
            'tokens_per_minute': 60000,  # High enough to not be limiting
            'request_burst_size': 2,
            'token_burst_size': 1000
        }

        limiter = DualRateLimiter(config)

        # First two should be instant (burst)
        start_time = time.time()
        limiter.wait_for_permission(10)
        limiter.wait_for_permission(10)
        burst_time = time.time() - start_time

        assert burst_time < 0.1  # Should be instant

        # Third should wait ~1 second
        start_time = time.time()
        limiter.wait_for_permission(10)
        wait_time = time.time() - start_time

        assert 0.9 < wait_time < 1.1  # Should wait approximately 1 second

    def test_token_rate_limiting(self):
        """Test that token rate limiting works correctly"""
        config = {
            'requests_per_minute': 6000,  # High enough to not be limiting
            'tokens_per_minute': 60,  # 1 per second
            'request_burst_size': 10,
            'token_burst_size': 2
        }

        limiter = DualRateLimiter(config)

        # First request should be instant
        start_time = time.time()
        limiter.wait_for_permission(2)  # Use all burst tokens
        burst_time = time.time() - start_time

        assert burst_time < 0.1  # Should be instant

        # Second should wait ~2 seconds (need 2 tokens at 1/second)
        start_time = time.time()
        limiter.wait_for_permission(2)
        wait_time = time.time() - start_time

        assert 1.8 < wait_time < 2.2  # Should wait approximately 2 seconds

    def test_concurrent_access(self):
        """Test that multiple threads can access the rate limiter safely"""
        config = {
            'requests_per_minute': 60,  # 1 per second
            'tokens_per_minute': 6000,  # 100 per second
            'request_burst_size': 2
        }

        limiter = DualRateLimiter(config)
        results = []

        def make_request(tokens):
            start = time.time()
            limiter.wait_for_permission(tokens)
            end = time.time()
            results.append(end - start)

        # Launch 5 threads
        threads = []
        for _ in range(5):
            t = threading.Thread(target=make_request, args=(50,))
            threads.append(t)
            t.start()

        for t in threads:
            t.join()

        # Sort results to analyze timing
        results.sort()

        # First 2 should be instant (burst), others should wait
        assert results[0] < 0.1
        assert results[1] < 0.1
        assert results[2] > 0.9  # Should wait ~1 second

    def test_stats_tracking(self):
        """Test that statistics are tracked correctly"""
        config = {
            'requests_per_minute': 60,
            'tokens_per_minute': 6000,
            'request_burst_size': 5
        }

        limiter = DualRateLimiter(config)

        # Make several requests
        limiter.wait_for_permission(100)
        limiter.wait_for_permission(200)

        stats = limiter.get_stats()

        assert stats['total_requests'] == 2
        assert stats['total_tokens'] == 300
        assert stats['total_wait_time'] >= 0
        assert stats['request_limited_count'] >= 0
        assert stats['token_limited_count'] >= 0

    def test_over_burst_request_raises_error(self):
        """Test that requesting more tokens than burst size raises an error"""
        config = {
            'requests_per_minute': 60,
            'tokens_per_minute': 6000,
            'token_burst_size': 100
        }

        limiter = DualRateLimiter(config)

        with pytest.raises(ValueError, match="Request requires 150 tokens but burst size is only 100"):
            limiter.wait_for_permission(150)

    def test_bucket_refill_timing(self):
        """Test that buckets refill correctly over time"""
        config = {
            'requests_per_minute': 60,  # 1 per second
            'tokens_per_minute': 60,    # 1 per second
            'request_burst_size': 1,
            'token_burst_size': 1
        }

        limiter = DualRateLimiter(config)

        # Consume all resources
        limiter.wait_for_permission(1)

        # Check that buckets are nearly empty (allowing for small timing differences)
        with limiter._lock:
            limiter._refill_buckets()
            assert limiter.request_bucket < 0.01  # Nearly empty
            assert limiter.token_bucket < 0.01    # Nearly empty

        # Wait 1 second and check refill
        time.sleep(1.1)
        with limiter._lock:
            limiter._refill_buckets()
            assert limiter.request_bucket >= 1.0
            assert limiter.token_bucket >= 1.0

    def test_thread_safety_of_stats(self):
        """Test that stats access is thread-safe"""
        config = {
            'requests_per_minute': 6000,
            'tokens_per_minute': 60000,
            'request_burst_size': 100
        }

        limiter = DualRateLimiter(config)

        def make_requests():
            for _ in range(10):
                limiter.wait_for_permission(10)

        def read_stats():
            for _ in range(20):
                stats = limiter.get_stats()
                assert isinstance(stats, dict)
                time.sleep(0.01)

        # Run concurrent operations
        threads = []
        for _ in range(3):
            threads.append(threading.Thread(target=make_requests))
        for _ in range(2):
            threads.append(threading.Thread(target=read_stats))

        for t in threads:
            t.start()
        for t in threads:
            t.join()

        # Verify final stats
        final_stats = limiter.get_stats()
        assert final_stats['total_requests'] == 30
        assert final_stats['total_tokens'] == 300


class TestTokenEstimator:
    """Test suite for estimate_tokens function"""

    def test_basic_token_estimation(self):
        """Test basic token estimation functionality"""
        text = "Hello, how are you today?"

        tokens = estimate_tokens("gpt-4", text)

        # Should return a positive integer
        assert isinstance(tokens, int)
        assert tokens > 0

        # Should include 15% buffer
        # Rough check: short text should be < 50 tokens with buffer
        assert tokens < 50

    def test_with_max_tokens(self):
        """Test token estimation with max_tokens specified"""
        text = "Short text"
        max_tokens = 1000

        tokens = estimate_tokens("gpt-4", text, max_tokens)

        # Should include completion tokens
        assert tokens > max_tokens  # Should be prompt + completion + buffer

    def test_long_text_estimation(self):
        """Test estimation with longer text"""
        # Create a longer text (approximately 100 words)
        text = " ".join(["word"] * 100)

        tokens = estimate_tokens("gpt-3.5-turbo", text)

        # Should scale with text length
        assert tokens > 50  # Should be significantly more than short text

    def test_fallback_estimation(self):
        """Test fallback estimation when tiktoken fails"""
        text = "Test text for fallback"

        with patch('tiktoken.get_encoding', side_effect=Exception("Mock error")):
            tokens = estimate_tokens("custom-model", text)

        # Should still return a positive integer
        assert isinstance(tokens, int)
        assert tokens > 0

    def test_empty_text(self):
        """Test estimation with empty text"""
        tokens = estimate_tokens("gpt-4", "")

        # Should handle empty text gracefully
        assert isinstance(tokens, int)
        assert tokens >= 0

    def test_different_models(self):
        """Test that estimation works with different model names"""
        text = "Test text"

        models = ["gpt-4", "gpt-3.5-turbo", "custom-model", "claude-3"]

        for model in models:
            tokens = estimate_tokens(model, text)
            assert isinstance(tokens, int)
            assert tokens > 0